{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Assignment\n",
    "\n",
    "* [link to d3 home page](https://titanbender.github.io) \n",
    "* [link to git repo](https://github.com/titanbender/titanbender.github.io) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/211653855\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x1060e9290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "VimeoVideo(\"211653855\",width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 20 seconds are intro and the last 12 seconds are outro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Used libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import collections \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import cross_validation\n",
    "import datetime as dt\n",
    "import geoplotlib\n",
    "from geoplotlib.utils import BoundingBox\n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create JSON function \n",
    "def createJSON(my_dict, file_name):\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(my_dict, outfile)\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(601313, 27)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data manipulation\n",
    "# Load data\n",
    "raw_DF = pd.read_csv(\"data/raw_data.csv\", low_memory = False)\n",
    "\n",
    "# Getting year\n",
    "year = raw_DF[\"DATE\"].apply(lambda x: x.split('/'))\n",
    "raw_DF[\"YEAR\"] = year.apply(lambda x: int(x[2]))\n",
    "\n",
    "# Getting hour\n",
    "hour = raw_DF[\"TIME\"].apply(lambda x: x.split(':'))\n",
    "raw_DF[\"HOUR\"] = hour.apply(lambda x: int(x[0]))\n",
    "raw_DF[\"TIME_DEC\"] = hour.apply(lambda x: float(int(x[0]) + int(x[1])/60) )\n",
    "\n",
    "# String values for month\n",
    "month_list = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
    "              'August', 'September', 'October', 'November', 'December']\n",
    "\n",
    "# Getting month\n",
    "month = raw_DF[\"DATE\"].apply(lambda x: x.split('/'))\n",
    "raw_DF[\"MONTH\"] = month.apply(lambda x: month_list[int(x[0])-1])\n",
    "\n",
    "# remove unused columns\n",
    "raw_DF = raw_DF.drop(['ZIP CODE',\n",
    "                          'LOCATION',\n",
    "                          'UNIQUE KEY',\n",
    "                          'ON STREET NAME',\n",
    "                          'CROSS STREET NAME',\n",
    "                          'OFF STREET NAME'], axis = 1)\n",
    "# raw_DF.head()\n",
    "\n",
    "raw_DF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pratical data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(raw_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pra_info = {}\n",
    "# First day of observations\n",
    "pra_info['first obs'] = min(raw_DF[\"DATE\"])\n",
    "# Last day of observations\n",
    "pra_info['last obs'] = max(raw_DF[\"DATE\"])\n",
    "# \n",
    "# pra_info['unique year'] = list(set(raw_DF['YEAR']))\n",
    "pra_info['unique year'] = [2013,2014,2015,2016]\n",
    "#\n",
    "pra_info['N'] = {}\n",
    "tmp = []\n",
    "for ii in pra_info['unique year']:\n",
    "    pra_info['N'][ii] = len(raw_DF[raw_DF['YEAR'] == ii])\n",
    "    tmp.append(pra_info['N'][ii])\n",
    "# sum tot\n",
    "pra_info['N']['tot'] = sum(tmp)\n",
    "\n",
    "pra_info['unique borough'] = list(set(raw_DF['BOROUGH']))\n",
    "pra_info['borough'] = {}\n",
    "for ii in pra_info['unique borough']:\n",
    "    pra_info['borough'][ii] = {}\n",
    "    tmp = []\n",
    "    for jj in pra_info['unique year']:\n",
    "        pra_info['borough'][ii][jj] = len(raw_DF[(raw_DF['YEAR'] == jj) & (raw_DF['BOROUGH'] == ii)])\n",
    "        tmp.append(pra_info['borough'][ii][jj])   \n",
    "    # sum tot\n",
    "    pra_info['borough'][ii]['tot'] = sum(tmp)\n",
    "    \n",
    "# obs per month\n",
    "pra_info['unique months'] = list(set(raw_DF['MONTH']))\n",
    "pra_info['N month'] = {}\n",
    "for ii in pra_info['unique months']:\n",
    "    pra_info['N month'][ii] = {}\n",
    "    tmp = []\n",
    "    for jj in pra_info['unique year']:\n",
    "        pra_info['N month'][ii][jj] = len(raw_DF[(raw_DF['YEAR'] == jj) & (raw_DF['MONTH'] == ii)])\n",
    "        tmp.append(pra_info['N month'][ii][jj])\n",
    "        \n",
    "    # sum tot\n",
    "    pra_info['N month'][ii]['tot'] = sum(tmp)\n",
    "    \n",
    "# ratio\n",
    "pra_info['ratio'] = {}\n",
    "for ii in pra_info['unique borough']:\n",
    "    pra_info['ratio'][ii] = {}\n",
    "    for jj in pra_info['unique year']+['tot']:\n",
    "        pra_info['ratio'][ii][jj] = pra_info['borough'][ii][jj] / pra_info['N'][jj] \n",
    "        \n",
    "# ratio month\n",
    "pra_info['ratio month'] = {}\n",
    "for ii in pra_info['unique months']:\n",
    "    pra_info['ratio month'][ii] = {}\n",
    "    for jj in pra_info['unique year']:\n",
    "        pra_info['ratio month'][ii][jj] = pra_info['N month'][ii][jj] / pra_info['N'][jj]\n",
    "        \n",
    "# obs per day\n",
    "pra_info['unique days'] = list(set(raw_DF['DATE']))\n",
    "pra_info['No. days'] = {}\n",
    "for ii in pra_info['unique days']:\n",
    "    pra_info['No. days'][ii] = len(raw_DF[raw_DF['DATE'] == ii])\n",
    "    \n",
    "# avg\n",
    "# counting\n",
    "tmp = [int(pra_info['No. days'].keys()[ii].split('/')[2]) for ii in range(0, len(pra_info['No. days']))]\n",
    "tmp_days = collections.Counter(tmp)\n",
    "tmp_days['tot'] = len(pra_info['No. days'])\n",
    "\n",
    "pra_info['avg day'] = {}\n",
    "for ii in pra_info['unique year']+['tot']:\n",
    "        pra_info['avg day'][ii] =  pra_info['N'][ii] / tmp_days[ii]\n",
    "        \n",
    "pra_info['avg year'] = {}\n",
    "for ii in pra_info['unique year']:\n",
    "        pra_info['avg year'][ii] =  pra_info['N'][ii] / len(pra_info['unique year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First day of observations: 01/01/2013\n",
      "Last day of observations: 12/31/2016\n",
      "Total number of obs. in mentioned period: 601313\n",
      "\n",
      "Observations per month:\n",
      "{'February': {2016: 12170, 'tot': 46477, 2013: 11246, 2014: 12142, 2015: 10919}, 'October': {2016: 10435, 'tot': 52628, 2013: 13417, 2014: 13773, 2015: 15003}, 'March': {2016: 13819, 'tot': 52484, 2013: 12809, 2014: 12127, 2015: 13729}, 'August': {2016: 565, 'tot': 40289, 2013: 12654, 2014: 12861, 2015: 14209}, 'September': {2016: 1157, 'tot': 41507, 2013: 12858, 2014: 13496, 2015: 13996}, 'December': {2016: 11421, 'tot': 51799, 2013: 13197, 2014: 13023, 2015: 14158}, 'June': {2016: 12277, 'tot': 54375, 2013: 14045, 2014: 13966, 2015: 14087}, 'April': {2016: 13765, 'tot': 51490, 2013: 12712, 2014: 12413, 2015: 12600}, 'May': {2016: 13880, 'tot': 56432, 2013: 14334, 2014: 13708, 2015: 14510}, 'January': {2016: 13945, 'tot': 51049, 2013: 11979, 2014: 12713, 2015: 12412}, 'November': {2016: 11372, 'tot': 51315, 2013: 13427, 2014: 12884, 2015: 13632}, 'July': {2016: 10839, 'tot': 51468, 2013: 13280, 2014: 13187, 2015: 14162}}\n",
      "\n",
      "Accidents per district:\n",
      "{'BRONX': {2016: 17482, 'tot': 78176, 2013: 19766, 2014: 19680, 2015: 21248}, 'BROOKLYN': {2016: 39032, 'tot': 184622, 2013: 47011, 2014: 47745, 2015: 50834}, 'STATEN ISLAND': {2016: 5519, 'tot': 26770, 2013: 8295, 2014: 6613, 2015: 6343}, 'MANHATTAN': {2016: 30448, 'tot': 155788, 2013: 41578, 2014: 41249, 2015: 42513}, 'QUEENS': {2016: 33164, 'tot': 155957, 2013: 39308, 2014: 41006, 2015: 42479}}\n",
      "\n",
      "Geographic area of each district:\n",
      "To be done.... \n",
      "\n",
      "Ratio between geographic area per district and number of accidents\n",
      "{'BRONX': {2016: 0.13913804767400215, 'tot': 0.13000883067553837, 2013: 0.12673925031098116, 2014: 0.12591734754595535, 2015: 0.13002319220154573}, 'BROOKLYN': {2016: 0.31065303036332526, 'tot': 0.30703144618526457, 2013: 0.30143371933469265, 2014: 0.30548393082223774, 2015: 0.3110692278037169}, 'STATEN ISLAND': {2016: 0.043925345218671655, 'tot': 0.04451924372165578, 2013: 0.05318739660677875, 2014: 0.04231155585982738, 2015: 0.038814811188554434}, 'MANHATTAN': {2016: 0.24233355883640414, 'tot': 0.2590797138927647, 2013: 0.2665974172533631, 2014: 0.26392096894934514, 2015: 0.26015041274775574}, 'QUEENS': {2016: 0.2639500179075968, 'tot': 0.2593607655247766, 2013: 0.25204221649418435, 2014: 0.2623661968226344, 2015: 0.2599423560584272}}\n",
      "\n",
      "Ratio between accidents and months\n",
      "{'February': {2016: 0.09686020136097735, 2013: 0.07210915759371113, 2014: 0.0776874204218999, 2015: 0.06681679384641745}, 'October': {2016: 0.0830514544948068, 2013: 0.08602957206427371, 2014: 0.08812294856455503, 2015: 0.09180807382340883}, 'March': {2016: 0.1099844800827729, 2013: 0.08213108657459059, 2014: 0.07759144683383133, 2015: 0.08401206728798105}, 'August': {2016: 0.004496796529905687, 2013: 0.08113722925402993, 2014: 0.08228775440998637, 2015: 0.0869493381961485}, 'September': {2016: 0.009208484221417486, 2013: 0.08244527372754203, 2014: 0.0863506363048889, 2015: 0.08564592423064919}, 'December': {2016: 0.09089896135938558, 2013: 0.08461893586734891, 2014: 0.08332426916112685, 2015: 0.08663725316215572}, 'June': {2016: 0.09771180707549047, 2013: 0.09005629720822272, 2014: 0.08935780873103721, 2015: 0.08620278184032261}, 'April': {2016: 0.10955469775956067, 2013: 0.08150912425140101, 2014: 0.07942134324633861, 2015: 0.07710336133939553}, 'May': {2016: 0.11046997492936447, 2013: 0.09190936021236487, 2014: 0.08770706301625793, 2015: 0.08879125182814518}, 'January': {2016: 0.11098730550360142, 2013: 0.07680914092255607, 2014: 0.08134081500770989, 2015: 0.07595293023369662}, 'November': {2016: 0.09050897369573004, 2013: 0.08609369189140666, 2014: 0.0824349139116915, 2015: 0.08341849379195555}, 'July': {2016: 0.08626686298698714, 2013: 0.08515113043255236, 2014: 0.08437358039067648, 2015: 0.08666173041972378}}\n",
      "\n",
      "Types of accidents\n",
      "To be done.... \n",
      "\n",
      "Average accidents per year:\n",
      "{2016: 31411.25, 2013: 38989.5, 2014: 39073.25, 2015: 40854.25}\n",
      "\n",
      "Average accidents per day:\n",
      "{2016: 343.29234972677597, 'tot': 411.5763175906913, 2013: 427.2821917808219, 2014: 428.2, 2015: 447.7178082191781}\n",
      "\n",
      "Day with most accidents\n",
      "('01/21/2014', 843)\n",
      "\n",
      "Day with least accidents\n",
      "('08/10/2016', 1)\n",
      "\n",
      "Top 10 days with most accidents:\n",
      "[('01/21/2014', 843), ('02/03/2014', 776), ('03/06/2015', 715), ('03/08/2013', 703), ('01/18/2015', 692), ('03/05/2015', 675), ('11/26/2013', 656), ('02/14/2014', 645), ('06/07/2013', 623), ('11/19/2015', 622)]\n",
      "\n",
      "Top 10 days with least accidents:\n",
      "[('08/10/2016', 1), ('08/14/2016', 2), ('08/22/2016', 3), ('08/12/2016', 3), ('08/31/2016', 3), ('08/17/2016', 4), ('09/25/2016', 4), ('08/23/2016', 4), ('08/06/2016', 5), ('08/21/2016', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Pratical data information prints\n",
    "print \"First day of observations:\", min(raw_DF[\"DATE\"])\n",
    "print \"Last day of observations:\", max(raw_DF[\"DATE\"])\n",
    "print \"Total number of obs. in mentioned period:\" , len(raw_DF['DATE'])\n",
    "print \"\"\n",
    "print \"Observations per month:\"\n",
    "print pra_info['N month']\n",
    "print \"\"\n",
    "print \"Accidents per district:\"\n",
    "print pra_info['borough']\n",
    "print \"\"\n",
    "print \"Geographic area of each district:\"\n",
    "print \"To be done.... \"\n",
    "print \"\"\n",
    "print \"Ratio between geographic area per district and number of accidents\"\n",
    "print pra_info['ratio']\n",
    "print \"\"\n",
    "print \"Ratio between accidents and months\"\n",
    "print pra_info['ratio month']\n",
    "print \"\"\n",
    "print \"Types of accidents\"\n",
    "print \"To be done.... \"\n",
    "print \"\"\n",
    "print \"Average accidents per year:\"\n",
    "print pra_info['avg year']\n",
    "print \"\"\n",
    "print \"Average accidents per day:\"\n",
    "print pra_info['avg day'] #pra_info['N']['tot'] / len(pra_info['No. days'])\n",
    "print \"\"\n",
    "print \"Day with most accidents\"\n",
    "print sorted(pra_info['No. days'].items(), key=lambda x: x[1], reverse=True)[0]\n",
    "print \"\"\n",
    "print \"Day with least accidents\"\n",
    "print sorted(pra_info['No. days'].items(), key=lambda x: x[1], reverse=False)[0]\n",
    "print \"\"\n",
    "print \"Top 10 days with most accidents:\" \n",
    "print sorted(pra_info['No. days'].items(), key=lambda x: x[1], reverse=True)[0:10]\n",
    "print \"\"\n",
    "print \"Top 10 days with least accidents:\" \n",
    "print sorted(pra_info['No. days'].items(), key=lambda x: x[1], reverse=False)[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Motivation\n",
    "- What is your dataset?\n",
    "- Why did you choose this/these particular dataset(s)?\n",
    "- What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic stats. Let's understand the dataset better\n",
    "- Write about your choices in data cleaning and preprocessing\n",
    "- Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Theory. Which theoretical tools did you use?\n",
    "- Describe which machine learning tools you use and why the tools you've chosen are right for the problem you're solving.\n",
    "- Talk about your model selection. How did you split the data in to test/training. Did you use cross validation?\n",
    "- Explain the model performance. How did you measure it? Are your results what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations\n",
    "- Explain the visualizations you've chosen.\n",
    "- Why are they right for the story you want to tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Time Series\n",
    "pra_info['ts_json'] = []\n",
    "            \n",
    "# loop for each day\n",
    "for dd in pra_info['unique days']:\n",
    "    # init dict\n",
    "    tmp = {}\n",
    "    \n",
    "    # add date\n",
    "    tmp['date'] = dd\n",
    "    \n",
    "    # sum number of tot_acc pr. day\n",
    "    tmp['tot_acc'] = len(raw_DF[raw_DF['DATE'] == dd])\n",
    "    \n",
    "    # loop for each borough\n",
    "    for bb in pra_info['unique borough']:\n",
    "        \n",
    "        # find indices for unique day and unique borough\n",
    "        tmp_idx = list(raw_DF[(raw_DF['DATE'] == dd) & (raw_DF['BOROUGH'] == bb)].index)\n",
    "        #\n",
    "        tmp[bb + '_ped_inj'] = np.sum(raw_DF['NUMBER OF PEDESTRIANS INJURED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_ped_kill'] = np.sum(raw_DF['NUMBER OF PEDESTRIANS KILLED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_ped_tot'] = (tmp[bb + '_ped_inj'] + \n",
    "                                tmp[bb + '_ped_kill'])\n",
    "        \n",
    "        tmp[bb + '_cyc_inj'] = np.sum(raw_DF['NUMBER OF CYCLIST INJURED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_cyc_kill'] = np.sum(raw_DF['NUMBER OF CYCLIST KILLED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_cyc_tot'] = (tmp[bb + '_cyc_inj'] + \n",
    "                                tmp[bb + '_cyc_kill'])\n",
    "        \n",
    "        tmp[bb + '_mot_inj'] = np.sum(raw_DF['NUMBER OF MOTORIST INJURED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_mot_kill'] = np.sum(raw_DF['NUMBER OF MOTORIST KILLED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_mot_tot'] = (tmp[bb + '_mot_inj'] + \n",
    "                                tmp[bb + '_mot_kill'])\n",
    "        \n",
    "        tmp[bb + '_tot_inj'] = np.sum(raw_DF['NUMBER OF PERSONS INJURED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_tot_kill'] = np.sum(raw_DF['NUMBER OF PERSONS KILLED'].iloc[tmp_idx])\n",
    "        tmp[bb + '_tot_tot'] = (tmp[bb + '_tot_inj'] + \n",
    "                                tmp[bb + '_tot_kill'])\n",
    "    # append day and info to the json list\n",
    "    pra_info['ts_json'].append(tmp)\n",
    "\n",
    "# create json\n",
    "tmp = pra_info['ts_json']\n",
    "tmp.sort(key=lambda x: dt.datetime.strptime(x['date'], '%m/%d/%Y'))\n",
    "createJSON(tmp,'data/ts_all.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "tmp_x_key = [\"00:00-00:59\",\"01:00-01:59\",\"02:00-02:59\",\"03:00-03:59\",\"04:00-04:59\",\"05:00-05:59\",\n",
    "             \"06:00-06:59\",\"07:00-07:59\",\"08:00-08:59\",\"09:00-09:59\",\"10:00-10:59\",\"11:00-11:59\",\n",
    "             \"12:00-12:59\",\"13:00-13:59\",\"14:00-14:59\",\"15:00-15:59\",\"16:00-16:59\",\"17:00-17:59\",\n",
    "             \"18:00-18:59\",\"19:00-19:59\",\"20:00-20:59\",\"21:00-21:59\",\"22:00-22:59\",\"23:00-23:59\"]\n",
    "# \n",
    "bar_dict_NYC = []\n",
    "bar_dict_BRONX = []\n",
    "bar_dict_BROOKLYN = []\n",
    "bar_dict_STATEN_ISLAND = []\n",
    "bar_dict_MANHATTAN = []\n",
    "bar_dict_QUEENS = []\n",
    "\n",
    "# \n",
    "for ii in range(0,24):\n",
    "    #print ii\n",
    "    \n",
    "    tmp_borough = {}\n",
    "    for jj in pra_info['unique borough']:\n",
    "        #print 's'\n",
    "        tmp_borough[jj] = len(raw_DF[(raw_DF['BOROUGH'] == jj) & (raw_DF['HOUR'] == ii)])\n",
    "        \n",
    "    # sum all for given time slot\n",
    "    tmp_sum = np.sum(tmp_borough.values())\n",
    "    \n",
    "    # append time slot to list\n",
    "    bar_dict_NYC.append({'key': tmp_x_key[ii], 'value': tmp_sum})\n",
    "    bar_dict_BRONX.append({'key': tmp_x_key[ii], 'value': tmp_borough['BRONX']})\n",
    "    bar_dict_BROOKLYN.append({'key': tmp_x_key[ii], 'value': tmp_borough['BROOKLYN']})\n",
    "    bar_dict_STATEN_ISLAND.append({'key': tmp_x_key[ii], 'value': tmp_borough['STATEN ISLAND']})\n",
    "    bar_dict_MANHATTAN.append({'key': tmp_x_key[ii], 'value': tmp_borough['MANHATTAN']})\n",
    "    bar_dict_QUEENS.append({'key': tmp_x_key[ii], 'value': tmp_borough['QUEENS']})\n",
    "\n",
    "    \n",
    "# create bar jsons \n",
    "createJSON(bar_dict_NYC, 'data/bar_dict_NYC.json')\n",
    "createJSON(bar_dict_BRONX, 'data/bar_dict_BRONX.json')\n",
    "\n",
    "createJSON(bar_dict_BROOKLYN, 'data/bar_dict_BROOKLYN.json')\n",
    "createJSON(bar_dict_STATEN_ISLAND, 'data/bar_dict_STATEN_ISLAND.json')\n",
    "\n",
    "createJSON(bar_dict_MANHATTAN, 'data/bar_dict_MANHATTAN.json')\n",
    "createJSON(bar_dict_QUEENS, 'data/bar_dict_QUEENS.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create bool variable KNN_bool = [0 or 1]. 0 if persons not are injured nor killed otherwise 1...\n",
    "raw_DF['KNN_bool'] = raw_DF[['NUMBER OF PERSONS INJURED',\n",
    "                             'NUMBER OF PERSONS KILLED']].apply(lambda x: 1 if (x['NUMBER OF PERSONS INJURED'] + \n",
    "                                                                                x['NUMBER OF PERSONS KILLED']) > 0 \n",
    "                                                                else 0, axis=1)\n",
    "\n",
    "# Create dict for prediction tool\n",
    "knn_dict = {}\n",
    "knn_dict['lat'] = list(raw_DF['LATITUDE'])\n",
    "knn_dict['lat_min_max'] = [np.min(knn_dict['lat']), np.max(knn_dict['lat'])]\n",
    "knn_dict['lon'] = list(raw_DF['LONGITUDE'])\n",
    "knn_dict['lon_min_max'] = [np.min(knn_dict['lon']), np.max(knn_dict['lon'])]\n",
    "knn_dict['time'] = list(raw_DF['TIME_DEC'])\n",
    "\n",
    "knn_dict['class'] = list(raw_DF['KNN_bool'])\n",
    "knn_dict['N'] = len(knn_dict['class'])\n",
    "\n",
    "# For prediction \n",
    "# todo\n",
    "#knn_dict['X'] = np.array([[knn_dict['lat'][ii], knn_dict['lon'][ii], knn_dict['time'][ii]] for ii in range(0,knn_dict['N'])])\n",
    "knn_dict['X'] = np.array([[knn_dict['lat'][ii], knn_dict['lon'][ii]] for ii in range(0,knn_dict['N'])])\n",
    "knn_dict['y'] = np.array([knn_dict['class'][ii] for ii in range(0,knn_dict['N'])])\n",
    "\n",
    "# create knn_json for D3\n",
    "knn_json = [{'lat': knn_dict['lat'][ii],\n",
    "             'lon': knn_dict['lon'][ii],\n",
    "             'class': knn_dict['class'][ii]} for ii in range(0,2000)] # todo\n",
    "             #'class': knn_dict['class'][ii]} for ii in range(0,len(knn_dict['class']))]\n",
    "\n",
    "createJSON(knn_json, 'data/knn_json.json') \n",
    "\n",
    "# \n",
    "tmp = collections.Counter(knn_dict['class'])\n",
    "print 'Non fatal accidents: {0:.3f}%'.format(tmp.values()[0] / np.sum(tmp.values()) * 100)\n",
    "print 'Fatal accidents:     {0:.3f}%'.format(tmp.values()[1] / np.sum(tmp.values()) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Estimate model parameters using cross validation\n",
    "\n",
    "# Maximum number of neighbors\n",
    "K = 50\n",
    "N = knn_dict['N']\n",
    "# K-fold crossvalidation\n",
    "K_fold = 10\n",
    "CV = cross_validation.KFold(N, K_fold, shuffle = False)\n",
    "#CV = cross_validation.LeaveOneOut(N)\n",
    "\n",
    "errors = np.zeros((N, K))\n",
    "i = 0\n",
    "for train_index, test_index in CV:\n",
    "    print 'Crossvalidation fold: {0}/{1}'.format(i + 1, CV.n_folds)\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = knn_dict['X'][train_index,:]\n",
    "    y_train = knn_dict['y'][train_index]\n",
    "    X_test = knn_dict['X'][test_index,:]\n",
    "    y_test = knn_dict['y'][test_index]\n",
    "\n",
    "    # Fit classifier and classify the test points (consider 1 to 40 neighbors)\n",
    "    for k in range(1, K + 1):\n",
    "        knclassifier = KNeighborsClassifier(n_neighbors = k);\n",
    "        knclassifier.fit(X_train, y_train);\n",
    "        y_est = knclassifier.predict(X_test);\n",
    "        errors[i, k - 1] = np.sum(y_est != y_test)\n",
    "    # \n",
    "    i +=1\n",
    "    \n",
    "# Plot the classification error rate\n",
    "plt.plot(range(1,errors.shape[1] + 1), (100 * np.sum(errors,0) / N).tolist())\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Classification error rate (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Denne her skal muligvis slettes...\n",
    "lat_min, lat_max = knn_dict['lat_min_max']\n",
    "lon_min, lon_max = knn_dict['lon_min_max']\n",
    "lat_mean = np.mean([lat_min,lat_max])\n",
    "lon_mean = np.mean([lon_min,lon_max])\n",
    "print '(',lat_mean,',', lon_mean,')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fitting KKN for the complate observations\n",
    "\n",
    "knn_dict['k_opt'] = np.argmin(np.sum(errors,0)) + 1\n",
    "\n",
    "\n",
    "knn_dict['knn_opt_fit'] = KNeighborsClassifier(n_neighbors = knn_dict['k_opt']);\n",
    "#knn_dict['knn_opt_fit'] = KNeighborsClassifier(n_neighbors = 2);\n",
    "\n",
    "knn_dict['knn_opt_fit'].fit(knn_dict['X'], knn_dict['y']);\n",
    "lat_min, lat_max = knn_dict['lat_min_max']\n",
    "lon_min, lon_max = knn_dict['lon_min_max']\n",
    "\n",
    "# Plot the decision boundary. For that, we will asign a color to each\n",
    "# create prediction grid\n",
    "# meshgrid size\n",
    "h = 0.0001\n",
    "h = 0.001\n",
    "#xx, yy = np.arange(lat_min, lat_max, h), np.arange(lon_min, lon_max, h)\n",
    "#pre = [[xx[ii], yy[jj]] for ii in range(0,len(xx)) for jj in range(0,len(yy))]\n",
    "\n",
    "\n",
    "# use complete data to classify based upon cross validated \n",
    "pre = knn_dict['X'].tolist()\n",
    "# prediction\n",
    "Z = knn_dict['knn_opt_fit'].predict(pre)\n",
    "\n",
    "# \n",
    "pre_knn_dict = {}\n",
    "pre_knn_dict['lat'] = [pre[ii][0] for ii in range(0,len(Z))]\n",
    "pre_knn_dict['lon'] = [pre[ii][1] for ii in range(0,len(Z))]\n",
    "#pre_knn_dict['time'] = [pre[ii][2] for ii in range(0,len(Z))]\n",
    "pre_knn_dict['class'] = [Z[ii] for ii in range(0,len(Z))]\n",
    "\n",
    "# json\n",
    "def r_col(x):\n",
    "    if x == 1:\n",
    "        tmp_col = 'rgba(255,0,0, 0.25)'\n",
    "    else:\n",
    "        tmp_col = 'rgba(0,255,0, 0.25)'\n",
    "    return tmp_col\n",
    "\n",
    "knn_opt_json = [{'lat': pre_knn_dict['lat'][ii],\n",
    "                 'lon': pre_knn_dict['lon'][ii],\n",
    "                 'class': pre_knn_dict['class'][ii]} for ii in range(0,len(pre_knn_dict['class']))]\n",
    "#\n",
    "createJSON(knn_opt_json, 'data/knn_opt_json.json') \n",
    "\n",
    "#\n",
    "tmp = collections.Counter(pre_knn_dict['class'])\n",
    "print 'Non fatal accidents: {0:.3f}%'.format(tmp.values()[0] / np.sum(tmp.values()) * 100)\n",
    "print 'Fatal accidents:     {0:.3f}%'.format(tmp.values()[1] / np.sum(tmp.values()) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IN_col_d_tree = [\n",
    "    'DATE',\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "    'TIME_DEC',\n",
    "    'MONTH',\n",
    "    # below maybe\n",
    "    'CONTRIBUTING FACTOR VEHICLE 1',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 2',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 3',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 4',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 5',\n",
    "    'VEHICLE TYPE CODE 1',\n",
    "    'VEHICLE TYPE CODE 2',\n",
    "    'VEHICLE TYPE CODE 3',\n",
    "    'VEHICLE TYPE CODE 4',\n",
    "    'VEHICLE TYPE CODE 5']\n",
    "\n",
    "\n",
    "OUT_col_d_tree = [\n",
    "    'NUMBER OF PERSONS INJURED',\n",
    "    'NUMBER OF PERSONS KILLED',\n",
    "    'NUMBER OF PEDESTRIANS INJURED',\n",
    "    'NUMBER OF PEDESTRIANS KILLED',\n",
    "    'NUMBER OF CYCLIST INJURED',\n",
    "    'NUMBER OF CYCLIST KILLED',\n",
    "    'NUMBER OF MOTORIST INJURED',\n",
    "    'NUMBER OF MOTORIST KILLED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare the to predictors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Heat Map\n",
    "import gmplot\n",
    "gmap = gmplot.GoogleMapPlotter(np.mean(raw_DF[\"LATITUDE\"]), np.mean(raw_DF[\"LONGITUDE\"]), 10.5)\n",
    "\n",
    "#gmap.plot(track['lat'], track['lon'], 'red', edge_width = 5)\n",
    "#gmap.scatter(more_lats, more_lngs, '#3B0B39', size=40, marker=False)\n",
    "#gmap.scatter(track['lat'], track['lon'], 'k', marker=True)\n",
    "gmap.heatmap(list(raw_DF[\"LATITUDE\"])[0:100000], list(raw_DF[\"LONGITUDE\"])[0:100000], threshold=5, radius=5)\n",
    "\n",
    "gmap.draw(\"data/heatmap[0-100000obs].html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_data = {}\n",
    "geo_data['lat'] = list(raw_DF[\"LATITUDE\"])\n",
    "geo_data['lon'] = list(raw_DF[\"LONGITUDE\"])\n",
    "\n",
    "\n",
    "\n",
    "#geoplotlib.kde(geo_data, cmap = 'jet', bw = 5, cut_below=1e-4)\n",
    "geoplotlib.kde(geo_data, bw = 2, cut_below=1e-4)\n",
    "\n",
    "bbox = BoundingBox(north = np.mean(raw_DF[\"LATITUDE\"]) + 0.232,\n",
    "                   west = np.mean(raw_DF[\"LONGITUDE\"]) - 0.232,\n",
    "                   south = np.mean(raw_DF[\"LATITUDE\"]) - 0.232,\n",
    "                   east = np.mean(raw_DF[\"LONGITUDE\"]) + 0.232)\n",
    "\n",
    "geoplotlib.set_bbox(bbox)\n",
    "#geoplotlib.tiles_provider('watercolor')\n",
    "#geoplotlib.inline()\n",
    "geoplotlib.savefig('data/heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create geojson\n",
    "def parseJson(year,file_name):\n",
    "    # number of obs. \n",
    "    tmp_DF = raw_DF[raw_DF[\"YEAR\"] == year]\n",
    "    #\n",
    "    tmp_lat = list(tmp_DF[\"LATITUDE\"])\n",
    "    tmp_lon = list(tmp_DF[\"LONGITUDE\"])\n",
    "    tmp_hour = list(tmp_DF[\"HOUR\"])\n",
    "    tmp_borough = list(tmp_DF[\"BOROUGH\"])\n",
    "    # \n",
    "    N_tot = len(tmp_lat)\n",
    "    \n",
    "    # output\n",
    "    out_dict = [{'lat': tmp_lat[ii],\n",
    "                 'lon': tmp_lon[ii],\n",
    "                 'r': 1,\n",
    "                 #'borough': tmp_borough[ii],\n",
    "                 #'hour': tmp_hour[ii],\n",
    "                 #'class': \"all\"\n",
    "                } for ii in range(0,N_tot)]\n",
    "    #\n",
    "    createJSON(out_dict, file_name)    \n",
    "\n",
    "parseJson(2013, 'data/geo_plot_2013.json')\n",
    "parseJson(2014, 'data/geo_plot_2014.json')\n",
    "parseJson(2015, 'data/geo_plot_2015.json')\n",
    "parseJson(2016, 'data/geo_plot_2016.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion. Think critically about your creation\n",
    "- What went well?,\n",
    "- What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some additional notes:\n",
    "- Make sure that you use references when they're needed and follow academic standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Questions to explore\n",
    "\n",
    "#1: Visualize the geographic location of the accidents occurances over the years\n",
    "\n",
    "#2: Visualize the geographic location of the accidents orrances based upon the vehicle type\n",
    "\n",
    "#3: Create a bar plot to visualize crime occurances from year to year also broken down into types of accidents \n",
    "\n",
    "#4: Analyze if there is any seasonality of other patterns correlated with time\n",
    "\n",
    "#5: Create a prediction tool that can be used to predict the type of accident based upon goegraphic location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_DF[['CONTRIBUTING FACTOR VEHICLE 1',\n",
    " 'CONTRIBUTING FACTOR VEHICLE 2',\n",
    " 'CONTRIBUTING FACTOR VEHICLE 3',\n",
    " 'CONTRIBUTING FACTOR VEHICLE 4',\n",
    " 'CONTRIBUTING FACTOR VEHICLE 5',\n",
    " 'VEHICLE TYPE CODE 1',\n",
    " 'VEHICLE TYPE CODE 2',\n",
    " 'VEHICLE TYPE CODE 3',\n",
    " 'VEHICLE TYPE CODE 4',\n",
    " 'VEHICLE TYPE CODE 5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collections.Counter(list(raw_DF['CONTRIBUTING FACTOR VEHICLE 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collections.Counter(list(raw_DF['BOROUGH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(list(raw_DF['BOROUGH']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
